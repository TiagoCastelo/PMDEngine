# üèòÔ∏è PMDEngine - Propriedade Machine Learning Engine

## üéØ Objetivo do Projeto

O **PMDEngine** √© um motor de processamento de dados e an√°lise preditiva desenhado para recolher, processar e analisar listagens imobili√°rias. Utiliza o framework Scrapy (com Playwright) para fazer web scraping incremental de sites de imobili√°rio e um motor de Machine Learning (ML) para classificar e prever o valor de im√≥veis.

O foco principal √© identificar oportunidades de mercado, comparando o pre√ßo pedido com o pre√ßo estimado pelo modelo de ML.

## üèóÔ∏è Arquitetura

O projeto est√° dividido em duas componentes principais:

1.  **MLEngine (Scrapy Crawler):** Respons√°vel por recolher os dados de forma eficiente.
2.  **MLEngine (ML Pipeline/API):** Respons√°vel pelo processamento, treino de modelos e servi√ßo de predi√ß√£o.

### 1. Web Scraping (Scrapy)

* **Framework:** Scrapy (Python)
* **Acelera√ß√£o/Renderiza√ß√£o:** `scrapy-playwright` para lidar com sites JavaScript-heavy.
* **Spider Exemplo:** `remax_spider.py`
* **Persist√™ncia:** `PostgresPipeline` para salvar os dados numa base de dados PostgreSQL.
* **Otimiza√ß√£o:** Implementa l√≥gica TTL (Time-To-Live) para scraping incremental, evitando recolher p√°ginas de detalhes de im√≥veis que n√£o mudaram de pre√ßo ou que foram visitadas recentemente.

### 2. Machine Learning

* **Modelos:** Utiliza modelos pr√©-treinados (e.g., `apartamento_model.pkl`, `moradia_model.pkl`, `terreno_model.pkl`) para prever o pre√ßo de diferentes tipos de im√≥veis.
* **API:** O ficheiro `api/main.py` sugere uma interface para servir estas predi√ß√µes, provavelmente usando o FastAPI (padr√£o para `main.py` em APIs Python).
* **Processamento:** O m√≥dulo `common/processing.py` cont√©m a l√≥gica de pr√©-processamento de dados para garantir que os inputs para o modelo est√£o formatados corretamente.
* **Treino:** O diret√≥rio `ML_Training/` indica a exist√™ncia de scripts para treinar e atualizar os modelos de ML.

## ‚öôÔ∏è Configura√ß√£o do Projeto

### Pr√©-requisitos

* Python 3.x
* Docker e Docker Compose (Recomendado para ambiente de produ√ß√£o/desenvolvimento)
* Playwright Browsers (instalados via `playwright install`)

### 1. Vari√°veis de Ambiente

Este projeto depende de vari√°veis de ambiente, particularmente para a base de dados e URLs iniciais de *crawling*.

Crie um ficheiro `.env` no diret√≥rio `MLEngine/docker/` (ou similar) com as seguintes vari√°veis:

| Vari√°vel | Descri√ß√£o | Exemplo de Valor |
| :--- | :--- | :--- |
| `PGDATABASE` | Nome da base de dados PostgreSQL. | `imoveis_db` |
| `PGUSER` | Utilizador da base de dados. | `user` |
| `PGPASSWORD` | Palavra-passe da base de dados. | `mypassword` |
| `PGHOST` | Host da base de dados. | `localhost` ou nome do servi√ßo Docker |
| `PGPORT` | Porta da base de dados. | `5432` |
| `START_URLS_LIST` | Lista JSON de URLs para iniciar o scraping. | `["https://www.remax.pt/imoveis/venda/apartamento?p=1"]` |

### 2. Instala√ß√£o e Execu√ß√£o (Sem Docker)

1.  **Instalar depend√™ncias:**
    ```bash
    pip install -r MLEngine/requirements.txt
    playwright install
    ```

2.  **Configurar e Iniciar a Base de Dados:**
    Assegure-se de que o PostgreSQL est√° a correr e que a base de dados est√° criada (com a tabela `imoveis` esperada pelo `PostgresPipeline`).

3.  **Executar o Crawler:**
    ```bash
    cd MLEngine/src/
    # Certifique-se de que as vari√°veis de ambiente (DB e START_URLS_LIST) est√£o definidas antes de executar!
    scrapy crawl remax_imovel 
    ```

### 3. Execu√ß√£o (Com Docker)

O diret√≥rio `MLEngine/docker/` cont√©m ficheiros para orquestra√ß√£o Docker:

1.  **Configurar `.env`** (conforme a sec√ß√£o acima).
2.  **Construir e Correr os Servi√ßos:**
    ```bash
    cd MLEngine/docker/
    docker-compose up --build
    ```
    (Isto deve inicializar a base de dados, a aplica√ß√£o Scrapy/ML, dependendo do seu `docker-compose.yml`).

## üìà Machine Learning

O pipeline de ML pode ser executado para atualizar os modelos preditivos:

* **Treino do Modelo:** Consulte `ML_Training/treino_modelo.py`.
* **Encontrar Oportunidades:** Consulte `ML_Training/encontrar_oportunidades.py` para o script que usa os modelos para analisar os dados recolhidos.

---

Espero que este `README` seja um bom ponto de partida para a documenta√ß√£o do seu projeto! Quer que eu adicione mais alguma sec√ß√£o ou detalhe?
